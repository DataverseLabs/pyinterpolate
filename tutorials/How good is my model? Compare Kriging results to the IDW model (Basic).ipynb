{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4055724",
   "metadata": {},
   "source": [
    "# Inverse Distance Weighting model vs Kriging\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. Read point data,\n",
    "2. Divide dataset into two sets: modeling and validation set,\n",
    "3. Perform IDW and evaluate it,\n",
    "4. Perform variogram modeling on the modeling set,\n",
    "5. Validate Kriging and compare Kriging and IDW validation results.\n",
    "\n",
    "## Level: Basic\n",
    "\n",
    "## Changelog\n",
    "\n",
    "| Date | Change description | Author |\n",
    "|------|--------------------|--------|\n",
    "| 2021-05-12 | First version of tutorial | @szymon-datalions |\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this tutorial we will learn about the one method of validation of our Kriging model. We'll compare it to the Inverse Distance Weighting function where the unknown point value is interpolated as the weighted mean of it's neighbours. Weights are assigned by the inverted distance raised to the n-th power.\n",
    "\n",
    "(1) **GENERAL FORM OF IDW**\n",
    "\n",
    "$$z(u) = \\frac{\\sum_{i}\\lambda_{i}*z_{i}}{\\sum_{i}\\lambda_{i}}$$,\n",
    "\n",
    "where:\n",
    "\n",
    "- $z(u)$: is the value at unknown location,\n",
    "- $i$: is a i-th known location,\n",
    "- $z_{i}$: is a value at known location $i$,\n",
    "- $\\lambda_{i}$: is a weight assigned to the known location $i$.\n",
    "\n",
    "(2) **WEIGHTING PARAMETER**\n",
    "\n",
    "$$\\lambda_{i} = \\frac{1}{d^{p}_{i}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $d$:  is a distance from known point $z_{i}$ to the unknown point $z(u)$,\n",
    "- $p$: is a hyperparameter which controls how strong is a relationship between known point and unknown point. You may set large $p$ if you want to show strong relationship between closest point and very weak influence of distant points. On the other hand, you may set small $p$ to emphasize fact that points are influencing each other with the same power irrespectively of their distance.\n",
    "\n",
    "---\n",
    "\n",
    "As you noticed **IDW** is a simple but powerful technique. Unfortunately it has major drawback: **we must set `p` - power - manually** and it isn't derived from the data and variogram. That's why it can be used for other tasks. Example is to use IDW as a baseline for comparison to the other techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a224fa8",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71be79cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/szymon/miniconda3/envs/pyintpk/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyinterpolate.idw import inverse_distance_weighting  # function for idw\n",
    "from pyinterpolate.io_ops import read_point_data\n",
    "from pyinterpolate.semivariance import calculate_semivariance  # experimental semivariogram\n",
    "from pyinterpolate.semivariance import TheoreticalSemivariogram  # theoretical models\n",
    "from pyinterpolate.kriging import Krige  # kriging models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb666bc",
   "metadata": {},
   "source": [
    "## 1) Read point data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e93c5805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15.1152409 , 52.76514556, 91.27559662],\n",
       "       [15.1152409 , 52.74279035, 96.54829407],\n",
       "       [15.1152409 , 52.71070647, 51.25455093],\n",
       "       ...,\n",
       "       [15.37034993, 52.68338343, 40.30933762],\n",
       "       [15.37034993, 52.67096386, 21.94326782],\n",
       "       [15.37034993, 52.64239886, 51.52513504]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dem = read_point_data('../sample_data/point_data/poland_dem_gorzow_wielkopolski', data_type='txt')\n",
    "dem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcee1cb",
   "metadata": {},
   "source": [
    "## 2) Divide dataset into two sets: modeling and validation set\n",
    "\n",
    "In this step we will divide our dataset into two sets:\n",
    "\n",
    "- modeling set (50%): points used for variogram modeling,\n",
    "- validation set (50%): points used for prediction and results validation.\n",
    "\n",
    "Baseline dataset will be divided randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fda0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 40% of rows (values) to test our model\n",
    "\n",
    "def create_model_validation_sets(dataset: np.array, frac=0.5):\n",
    "    \"\"\"\n",
    "    Function divides base dataset into modeling and validation sets\n",
    "    \n",
    "    INPUT:\n",
    "    :param dataset: (numpy array) array with rows of records,\n",
    "    :param frac: (float) number of elements in a test set\n",
    "    \n",
    "    OUTPUT:\n",
    "    return: modeling_set (numpy array), validation_set (numpy array)\n",
    "    \"\"\"\n",
    "\n",
    "    removed_idx = np.random.randint(0, len(dem)-1, size=int(frac * len(dem)))\n",
    "    validation_set = dem[removed_idx]\n",
    "    modeling_set = np.delete(dem, removed_idx, 0)\n",
    "    return modeling_set, validation_set\n",
    "\n",
    "known_points, unknown_points = create_model_validation_sets(dem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27a2f14",
   "metadata": {},
   "source": [
    "## 3) Perform IDW and evaluate it\n",
    "\n",
    "Inverse Distance Weighting doesn't require variogram modeling or other steps. We pass power to which we want raise distance in weight denominator. Things to remember are:\n",
    "\n",
    "- Large `power` -> closer neighbours are more important,\n",
    "- `power` which is close to the **zero** -> all neighbours are important and we assume that distant process has the same effect on our variable as the closest events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "719ceb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDW_POWER = 2\n",
    "NUMBER_OF_NEIGHBOURS = -1  # Include all points in weighting process (equation 1)\n",
    "\n",
    "idw_predictions = []\n",
    "\n",
    "for pt in unknown_points:\n",
    "    idw_result = inverse_distance_weighting(known_points, pt[:-1], NUMBER_OF_NEIGHBOURS, IDW_POWER)\n",
    "    idw_predictions.append(idw_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01192717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error of prediction with IDW is 5.2961115785807005\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "idw_rmse = np.mean(np.sqrt((unknown_points[:, -1] - np.array(idw_predictions))**2))\n",
    "print(f'Root Mean Squared Error of prediction with IDW is {idw_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7c17e9",
   "metadata": {},
   "source": [
    "**Clarification:** Obtained Root Mean Squared Error could serve as a baseline for further model development. To build better reference, we create four IDW models of powers:\n",
    "\n",
    "1. 0.5,\n",
    "2. 1,\n",
    "3. 2,\n",
    "4. 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52a73e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "IDW_POWERS = [0.5, 1, 2, 4]\n",
    "idw_rmse = {}\n",
    "\n",
    "for pw in IDW_POWERS:\n",
    "    results = []\n",
    "    for pt in unknown_points:\n",
    "        idw_result = inverse_distance_weighting(known_points, pt[:-1], NUMBER_OF_NEIGHBOURS, pw)\n",
    "        results.append(idw_result)\n",
    "    idw_rmse[pw] = np.mean(np.sqrt((unknown_points[:, -1] - np.array(results))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7de725",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
