{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Blocks to points Ordinary Kriging interpolation - tutorial\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. Read block data,\n",
    "2. Detect and remove outliers,\n",
    "3. Create semivariogram model,\n",
    "4. Read point data canvas,\n",
    "5. Build map of interpolated values,\n",
    "6. Show map of interpolated values with choropleth map of the breast cancer rates.\n",
    "\n",
    "## Level: Intermediate\n",
    "\n",
    "## Changelog\n",
    "\n",
    "| Date | Change description | Author |\n",
    "|------|-------------------------------------|--------|\n",
    "| 2022-08-23 | Tutorial updated for the 0.3.0 version of the package | @SimonMolinsky |\n",
    "| 2021-12-14 | Sill selection was upgraded: now optimal sill is derived from the grid search within `TheoreticalSemivariogram` class | @SimonMolinsky |\n",
    "| 2021-12-13 | Changed behavior of `select_values_in_range()` function | @SimonMolinsky |\n",
    "| 2021-12-06 | Behavior of `prepare_kriging_data()` function has changed | @SimonMolinsky |\n",
    "| 2021-10-13 | Refactored TheoreticalSemivariogram (name change of class attribute) and refactored `calc_semivariance_from_pt_cloud()` functions to protect calculations from `NaN's`. | @ethmtrgt & @SimonMolinsky |\n",
    "| 2021-05-11 | Refactored TheoreticalSemivariogram class | @SimonMolinsky |\n",
    "| 2021-03-31 | Update related to the change of semivariogram weighting. Updated cancer rates data. | @SimonMolinsky |\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial is based on the question of user *@ikey* from *Discord* channel of the package (02/04/2021). Thank you *@ikey* for your engagement!\n",
    "\n",
    "In short:\n",
    "\n",
    "> Are we able to perform Semivariogram Regularization and Poisson Kriging if our point support represents only points without any values?\n",
    "\n",
    "**No, it's not possible**. But there is a hack that we can use - we can interpolate missing values from areal centroids with Ordinary and Simple Kriging just like we do with *regular points*. Why is it not possible with unknown points?\n",
    "\n",
    "The idea behind semivariogram regularization of areal aggregates is to use semivariogram of point support. Point support must reflect an ongoing process. For example, in the case of epidemiology:\n",
    "\n",
    "1. We have areal counts of infections divided by POPULATION over some areas.\n",
    "2. We take POPULATION blocks (points) with a number of inhabitants per block and use those as support for our map. Thus we transform the choropleth map into the point map, and only to the points where someone is living ==> someone may be infected. We skip large parts of the map where the risk of infection is not present because no one is living there.\n",
    "3. We finally obtain the population-at-risk map of points or smoothed choropleth map of infection risk without large visual bias where large/small areas seem to be more important.\n",
    "\n",
    "This works when we have specific **counts over area divided by another parameter** (time, population, probability, volume, etc.). In this scenario, we can find a function that links our areal counts with those underlying variables and/or processes. Without support values, we are not able to perform semivariogram regularization.\n",
    "\n",
    "What we want to achieve may be done with Ordinary Kriging (because our points are unknown), and semivariogram regularization is not required (and not possible without any values).\n",
    "\n",
    "In summary, we will learn how to:\n",
    "\n",
    "- transform areal data into points,\n",
    "- detect and remove outliers from data,\n",
    "- create an interpolated map from unknown points,\n",
    "- visualize and save the created map.\n",
    "\n",
    "We use:\n",
    "\n",
    "- for point canvas: `samples/point_data/shapefile/regular_grid_points.shp`,\n",
    "- for areal centroids: Breast cancer rates data is stored in the shapefile  `samples/regularization/cancer_data.gpkg`.\n",
    "\n",
    "This tutorial requires understanding concepts presented in **Semivariogram Estimation** and **Variogram Point Cloud** tutorials along with **Ordinary and Simple Kriging** notebook and (optionally) **Semivariogram Regularization** tutorial.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyinterpolate import Blocks\n",
    "from pyinterpolate.variogram.empirical import VariogramCloud\n",
    "from pyinterpolate.variogram import TheoreticalVariogram\n",
    "from pyinterpolate.kriging.point_kriging import kriging"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) Read areal data\n",
    "\n",
    "The block data represents the breast cancer rates in the Northeastern part of the United States. Rate is the number of new cases divided by the population in a county, and this fraction is multiplied by a constant factor of 100,000.\n",
    "\n",
    "We read data from `geopackage` and use only `rate`, `centroid.x`, and `centroid.y` columns."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read and prepare data\n",
    "\n",
    "DATASET = 'samples/regularization/cancer_data.gpkg'\n",
    "POLYGON_LAYER = 'areas'\n",
    "GEOMETRY_COL = 'geometry'\n",
    "POLYGON_ID = 'FIPS'\n",
    "POLYGON_VALUE = 'rate'\n",
    "MAX_RANGE = 400000\n",
    "STEP_SIZE = 40000\n",
    "\n",
    "AREAL_INPUT = Blocks()\n",
    "AREAL_INPUT.from_file(DATASET, value_col=POLYGON_VALUE, index_col=POLYGON_ID, layer_name=POLYGON_LAYER)\n",
    "\n",
    "AREAL_INPUT.data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "areal_centroids = AREAL_INPUT.data[['centroid.x', 'centroid.y', 'rate']].values\n",
    "areal_centroids[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) Detect and remove outliers\n",
    "\n",
    "We check and clean data before the semivariogram fitting and the Ordinary Kriging interpolation. Variogram Point Cloud is the best way to analyze data and detect outliers. We inspect and compare different lags and step sizes and their respective point clouds. Then we will create multiple variograms and Kriging models."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create analysis parameters\n",
    "\n",
    "maximum_range = 300000\n",
    "\n",
    "number_of_lags = [4, 8, 16]\n",
    "step_sizes = [maximum_range / x for x in number_of_lags]\n",
    "\n",
    "variogram_clouds = []\n",
    "\n",
    "for step_size in step_sizes:\n",
    "    vc = VariogramCloud(input_array=areal_centroids, step_size=step_size, max_range=maximum_range+step_size)\n",
    "    variogram_clouds.append(vc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for vc in variogram_clouds:\n",
    "    print(f'Lags per area: {len(vc.lags)}')\n",
    "    print('')\n",
    "    vc.plot('violin')\n",
    "    print('\\n###########\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does a fast check tell us?\n",
    "\n",
    "- point to point errors per lag are skewed toward positive values (the mean is always greater than the median),\n",
    "- less lags == less variability,\n",
    "- variability in the last case (32 lags) seems to be too high,\n",
    "- variability in the first case (4 lags) seems to be too small,\n",
    "- there are extreme semivariance values, and the largest extremities are present for the most distant lags.\n",
    "\n",
    "What can we do?\n",
    "\n",
    "- remove outliers (extremely high semivariances) per each lag,\n",
    "- use a small number of neighbors and a range close to 200,000 to avoid unnecessary computations."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now remove outliers from each lag\n",
    "\n",
    "_ = [vc.remove_outliers(method='iqr', iqr_lower_limit=3, iqr_upper_limit=1.5, inplace=True) for vc in variogram_clouds]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# And show data without outliers\n",
    "\n",
    "for vc in variogram_clouds:\n",
    "    print(f'Lags per area: {len(vc.lags)}')\n",
    "    print('')\n",
    "    vc.plot('violin')\n",
    "    print('\\n###########\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we compare the **y axis** before and after removing outliers, we may see that values are now smaller and densely populated. After outlier removal, we see also that 32 lags are too much, and especially for the first lag, it generates too much noise. We can expect that model based on this division will work poorly."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Create a theoretical semivariogram model\n",
    "\n",
    "Semivariogram may be fitted manually or automatically. In this case, we fit it automatically - we test multiple models, and it's easier to use the `autofit()` method of the `TheoreticalVariogram` object. We didn't force any model to see how different results we would get."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "theoretical_semivariograms = []\n",
    "\n",
    "for idx, vc in enumerate(variogram_clouds):\n",
    "    \n",
    "    print(f'Semivariance calculated for {vc.lags} lags.')\n",
    "    print('')\n",
    "    # Calculate experimental model\n",
    "    exp_model = vc.calculate_experimental_variogram()\n",
    "    \n",
    "    # Assign experimental model and data to TheoreticalSemivariogram\n",
    "\n",
    "    theo_semi = TheoreticalVariogram()\n",
    "    theo_semi.autofit(experimental_variogram=exp_model)\n",
    "    theoretical_semivariograms.append(theo_semi)\n",
    "    print('')\n",
    "    print('Model parameters:')\n",
    "    print('Model type:', theo_semi.name)\n",
    "    print('Nugget:', theo_semi.nugget)\n",
    "    print('Sill:', theo_semi.sill)\n",
    "    print('Range:', theo_semi.rang)\n",
    "    print('Model error:', theo_semi.rmse)\n",
    "    print('')\n",
    "    print('#####')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each case has a different model type, a different sill, and a different range! How do we choose the model parameters appropriately in this scenario? Error rises with the number of lags but is it a good indicator of the semivariogram fit? No, and we should be careful when choosing variograms with a few lags instead of variograms with multiple lags. We may miss some spatial patterns that will be averaged with a smaller number of lags. The 8-lag variogram seems to be the best because RMSE is the lowest. On the other hand, 32-lags variograms work poorly!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Read point data canvas\n",
    "\n",
    "Our variogram model is ready to load the point file as the **GeoDataFrame** object, which stores the geometry column and data features as tables."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "points = 'samples/point_data/shapefile/regular_grid_points.shp'  # file with grid for analysis\n",
    "gdf_pts = gpd.read_file(points)\n",
    "gdf_pts.set_index('id', inplace=True)\n",
    "gdf_pts['x'] = gdf_pts.geometry.x\n",
    "gdf_pts['y'] = gdf_pts.geometry.y\n",
    "gdf_pts.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Build a map of interpolated values\n",
    "\n",
    "We can model all values in a batch. If we set the `number_of_workers` parameter to `-1` or a positive integer, then the algorithm will use parallel processing functions to speed up calculations.\n",
    "\n",
    "Based on the learning from a semivariogram modeling step, we will set a number of neighboring areas to 8 and range to 200000. We will append interpolated values and errors to an existing data frame and plot them to compare results."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preds_cols = []\n",
    "errs_cols = []\n",
    "\n",
    "for semi_model in theoretical_semivariograms:\n",
    "    kriged = kriging(observations=areal_centroids,\n",
    "                     theoretical_model=semi_model,\n",
    "                     points=gdf_pts[['x', 'y']].values,\n",
    "                     neighbors_range=200000,\n",
    "                     no_neighbors=8,\n",
    "                     number_of_workers=1,\n",
    "                     use_all_neighbors_in_range=True)\n",
    "    \n",
    "    # Interpolate missing values and uncertainty\n",
    "    pred_col_name = 'p ' + semi_model.name[:3] + ' ' + str(len(semi_model.lags))\n",
    "    uncertainty_col_name = 'e ' + semi_model.name[:3] + ' ' + str(len(semi_model.lags))\n",
    "    gdf_pts[pred_col_name] = kriged[:, 0]\n",
    "    gdf_pts[uncertainty_col_name] = kriged[:, 1]\n",
    "    preds_cols.append(pred_col_name)\n",
    "    errs_cols.append(uncertainty_col_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gdf_pts.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is the somewhat complicated function in which we interpolate missing values as new columns of **GeoDataFrame**. *Uncertainty* is assigned to interpolated results for further analysis.\n",
    "\n",
    "Now we can save our **GeoDataFrame** as a shapefile."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save interpolation results\n",
    "\n",
    "gdf_pts.to_file('output/interpolation_results_areal_to_point.shp')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we will check the results directly in the notebook."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Show a map of interpolated values with a choropleth map of the breast cancer rates"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Now compare results to choropleth maps\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(16, 16), sharex='all', sharey='all')\n",
    "\n",
    "base1 = AREAL_INPUT.data.plot(ax=axes[0, 0], legend=True, edgecolor='black', color='white')\n",
    "base2 = AREAL_INPUT.data.plot(ax=axes[0, 1], legend=True, edgecolor='black', color='white')\n",
    "base3 = AREAL_INPUT.data.plot(ax=axes[0, 2], legend=True, edgecolor='black', color='white')\n",
    "base4 = AREAL_INPUT.data.plot(ax=axes[1, 0], legend=True, edgecolor='black', color='white')\n",
    "base5 = AREAL_INPUT.data.plot(ax=axes[1, 1], legend=True, edgecolor='black', color='white')\n",
    "base6 = AREAL_INPUT.data.plot(ax=axes[1, 2], legend=True, edgecolor='black', color='white')\n",
    "\n",
    "gdf_pts.plot(ax=base1, column=preds_cols[0], cmap='Spectral_r', alpha=0.3)\n",
    "gdf_pts.plot(ax=base2, column=preds_cols[1], cmap='Spectral_r', alpha=0.3)\n",
    "gdf_pts.plot(ax=base3, column=preds_cols[2], cmap='Spectral_r', alpha=0.3)\n",
    "gdf_pts.plot(ax=base4, column=errs_cols[0], cmap='Reds', alpha=0.3)\n",
    "gdf_pts.plot(ax=base5, column=errs_cols[1], cmap='Reds', alpha=0.3)\n",
    "gdf_pts.plot(ax=base6, column=errs_cols[2], cmap='Reds', alpha=0.3)\n",
    "\n",
    "axes[0, 0].set_title(preds_cols[0])\n",
    "axes[0, 1].set_title(preds_cols[1])\n",
    "axes[0, 2].set_title(preds_cols[2])\n",
    "axes[1, 0].set_title(errs_cols[0])\n",
    "axes[1, 1].set_title(errs_cols[1])\n",
    "axes[1, 2].set_title(errs_cols[2])\n",
    "\n",
    "plt.suptitle('Comparison of different Kriging models outputs')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visual inspection shows that:\n",
    "\n",
    "> The circular model and the exponential model have created the smoothest results. An interesting pattern emerges on uncertainty maps. The variance errors are smoother on the first map (the circular model). The middle map produces a higher variance in output, but it still works as a filter. The last model doesn't perform well, has too strict distance constraints, and the final outcome is not smooth. Sharply increasing variance errors on the uncertainty map of the cubic model tell us that maybe we have chosen the wrong model - we are not able to retrieve meaningful information from it.\n",
    "\n",
    "Are absolute errors between maps large or not? Let's check it in the last step:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for pcol1 in preds_cols:\n",
    "    print('Column:', pcol1)\n",
    "    for pcol2 in preds_cols:\n",
    "        if pcol1 == pcol2:\n",
    "            pass\n",
    "        else:\n",
    "            mad = gdf_pts[pcol1] - gdf_pts[pcol2]\n",
    "            mad = np.abs(np.mean(mad))\n",
    "            print(f'Mean Absolute Difference with {pcol2} is {mad:.4f}')\n",
    "    print('')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "That differences are tiny, so we shouldn't throw away one model after another. Semivariogram models are comparable. I prefer the **8-lag** theoretical variogram with an exponential model, and it has the best bias-to-variance ratio and a low RMSE error."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Where to go?\n",
    "\n",
    "### Next steps:\n",
    "\n",
    "* [How good is our Kriging Model]()\n",
    "* [Semivariogram Regularization]()\n",
    "\n",
    "### Additional materials:\n",
    "\n",
    "* [Ordinary and Simple Kriging]()\n",
    "* [Outliers and Their Influence on the Final Model]()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}