{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Poisson Kriging - Area to Area Kriging\n",
    "\n",
    "## Table of Contents:\n",
    "\n",
    "1. Load areal and point data,\n",
    "2. Load semivariogram (regularized),\n",
    "3. Remove 10% of areal dataset,\n",
    "4. Predict values at unknown locations,\n",
    "5. Analyse Forecast Bias and Root Mean Squared Error of prediction.\n",
    "\n",
    "## Level: Advanced\n",
    "\n",
    "## Changelog\n",
    "\n",
    "| Date | Change description                                                                                                    | Author |\n",
    "|------|-----------------------------------------------------------------------------------------------------------------------|--------|\n",
    "| 2022-08-27 | The tutorial was updated for the 0.3.0 version of the package                                                         | @SimonMolinsky |\n",
    "| 2021-12-14 | Sill selection was upgraded: now optimal sill is derived from the grid search within `TheoreticalSemivariogram` class | @SimonMolinsky |\n",
    "| 2021-12-13 | Changed behavior of `select_values_in_range()` function                                                               | @SimonMolinsky |\n",
    "| 2021-12-11 | Behavior of `prepare_kriging_data()` function has benn changed                                                        | @SimonMolinsky |\n",
    "| 2021-05-28 | Updated paths for input/output data                                                                                   | @SimonMolinsky |\n",
    "| 2021-05-11 | Refactored TheoreticalSemivariogram class                                                                             | @SimonMolinsky |\n",
    "| 2021-03-31 | Update related to the change of semivariogram weighting. Updated cancer rates data.                                   | @SimonMolinsky |\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Before starting this tutorial, be sure you understand concepts in the **Ordinary and Simple Kriging** and **Semivariogram Regularization** tutorials.\n",
    "\n",
    "The Poisson Kriging technique is used to model spatial count data. We will analyze a particular case where values are counted over blocks, and those blocks may have irregular shapes and sizes. We will try to predict the breast cancer rates in the Northeastern counties of the U.S. We will use U.S. Census 2010 data to create point support for blocks.\n",
    "\n",
    "> The breast cancer rates data and the point support population counts are located in the geopackage in a directory: `samples/regularization/cancer_data.gpkg`\n",
    "\n",
    "In tutorial **Poisson Kriging - Centroid based approach** we've assumed that all areas have the same size and shape. It is not a valid statement. Now we use the Area-to-Area Kriging technique and predict and evaluate missing values.\n",
    "\n",
    "Area-to-Area and Area-to-Point Poisson Kriging are slower than simplified Kriging with Centroids but give more reliable results because they are tuned to areal size and shape.\n",
    "\n",
    "This tutorial covers the following steps:\n",
    "\n",
    "1. Read and explore data,\n",
    "2. Load semivariogram model,\n",
    "3. Prepare training and test data,\n",
    "4. Predict values of unknown locations and calculate forecast bias and root mean squared error,\n",
    "5. Analyze error metrics."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1) Read and explore data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyinterpolate import TheoreticalVariogram\n",
    "from pyinterpolate import Blocks, PointSupport\n",
    "from pyinterpolate import area_to_area_pk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATASET = 'samples/regularization/cancer_data.gpkg'\n",
    "OUTPUT = 'samples/regularization/regularized_variogram.json'\n",
    "POLYGON_LAYER = 'areas'\n",
    "POPULATION_LAYER = 'points'\n",
    "POP10 = 'POP10'\n",
    "GEOMETRY_COL = 'geometry'\n",
    "POLYGON_ID = 'FIPS'\n",
    "POLYGON_VALUE = 'rate'\n",
    "\n",
    "blocks = Blocks()\n",
    "blocks.from_file(DATASET, value_col=POLYGON_VALUE, index_col=POLYGON_ID, layer_name=POLYGON_LAYER)\n",
    "\n",
    "point_support = PointSupport()\n",
    "point_support.from_files(point_support_data_file=DATASET,\n",
    "                         blocks_file=DATASET,\n",
    "                         point_support_geometry_col=GEOMETRY_COL,\n",
    "                         point_support_val_col=POP10,\n",
    "                         blocks_geometry_col=GEOMETRY_COL,\n",
    "                         blocks_index_col=POLYGON_ID,\n",
    "                         use_point_support_crs=True,\n",
    "                         point_support_layer_name=POPULATION_LAYER,\n",
    "                         blocks_layer_name=POLYGON_LAYER)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Lets take a look into a map of areal counts\n",
    "\n",
    "blocks.data.plot(column=blocks.value_column_name, cmap='Spectral_r', legend=True, figsize=(12, 8))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Clarification**: It is a good idea to look into the spatial patterns in a dataset and to visually check if our data do not have any NaN values. We use the geopandas `GeoDataFrame.plot()` function with a color map that diverges regions based on the cancer incidence rates. The output choropleth map is not ideal, and (probably) it has a few unreliable results, for example:\n",
    "\n",
    "- in counties with a small population, where the ratio number of cases to population size is high even if the number of cases is low,\n",
    "- counties that are very big and sparsely populated may draw more of our attention instead of densely populated counties,\n",
    "- transitions of colors (rates) between counties may be too abrupt, even if we know, that neighboring counties should have closer results.\n",
    "\n",
    "We can overcome those problems using Area-to-Area Kriging for filtering and denoising. But, in this tutorial, we will make an interpolation with Area-to-Area Kriging and analyze root mean squared errors of predictions and their bias."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2) Load a semivariogram model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this step, we load regularized semivariogram from the tutorial **Semivariogram Regularization (Intermediate)**. You can always perform semivariogram regularization along with the Poisson Kriging, but it is a very long process, and it is more convenient to separate those two steps."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "semivariogram = TheoreticalVariogram()  # Create TheoreticalSemivariogram object\n",
    "semivariogram.from_json('output/regularized_model.json')  # Load regularized semivariogram"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3) Prepare training and test data.\n",
    "\n",
    "We simply remove 40% of random ID's from the areal dataset (the same for points) and create four arrays: two training arrays with areal and point geometry and values and two test arrays with the same categories of data."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove 40% of rows (values) to test our model\n",
    "\n",
    "def create_test_areal_set(areal_dataset: Blocks, points_dataset: PointSupport, frac=0.4):\n",
    "    \"\"\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    areal_dataset : Blocks\n",
    "    points_dataset : PointSupport\n",
    "    frac : float\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    : List\n",
    "        [[training_areas, test_areas, training_points, test_points]]\n",
    "            equal to:\n",
    "        [np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n",
    "            where:\n",
    "        - areas: [[block index, centroid x, centroid y, value]]\n",
    "        - point support: [[block id, x, y, value]]\n",
    "    \"\"\"\n",
    "    block_id_col = areal_dataset.index_column_name\n",
    "    block_data = areal_dataset.data.copy()\n",
    "    all_ids = block_data[block_id_col].unique()\n",
    "    training_set_size = int(len(all_ids) * (1 - frac))\n",
    "\n",
    "    training_ids = np.random.choice(all_ids,\n",
    "                                    size=training_set_size,\n",
    "                                    replace=False)\n",
    "\n",
    "    training_areas = block_data[block_data[block_id_col].isin(training_ids)]\n",
    "    training_areas = training_areas[[block_id_col, 'centroid.x', 'centroid.y', areal_dataset.value_column_name]].values\n",
    "    test_areas = block_data[~block_data[block_id_col].isin(training_ids)]\n",
    "    test_areas = test_areas[[block_id_col, 'centroid.x', 'centroid.y', areal_dataset.value_column_name]].values\n",
    "\n",
    "    ps_data = points_dataset.point_support.copy()\n",
    "    ps_ids = points_dataset.block_index_column\n",
    "\n",
    "    training_points = ps_data[ps_data[ps_ids].isin(training_ids)]\n",
    "    training_points = training_points[[ps_ids,\n",
    "                                       points_dataset.x_col,\n",
    "                                       points_dataset.y_col,\n",
    "                                       points_dataset.value_column]].values\n",
    "    test_points = ps_data[~ps_data[ps_ids].isin(training_ids)]\n",
    "    test_points = test_points[[ps_ids,\n",
    "                               points_dataset.x_col,\n",
    "                               points_dataset.y_col,\n",
    "                               points_dataset.value_column]].values\n",
    "\n",
    "\n",
    "    output = [training_areas, test_areas, training_points, test_points]\n",
    "    return output\n",
    "\n",
    "ar_train, ar_test, pt_train, pt_test = create_test_areal_set(blocks, point_support)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4) Predict values at unknown locations and calculate forecast bias and root mean squared error.\n",
    "\n",
    "Do you know scikit-learn's fit-predict-transform estimation procedure? With Kriging, it is more complicated because we fit the semivariogram model to data, and we pass this fitted model into the kriging estimator.\n",
    "\n",
    "You may start work with predictions with a fitted semivariogram model. The run of Area-to-Area Poisson Kriging requires five core arguments:\n",
    "\n",
    "- semivariogram model (fitted semivariogram model),\n",
    "- known areas (training set),\n",
    "- known points (training set),\n",
    "- unknown-block (without rate value),\n",
    "- unknown block’s point support.\n",
    "\n",
    "Additional parameters control the interpolation process:\n",
    "\n",
    "- **number of observations** (the most critical parameter - how many neighbors are affecting your analysis area).\n",
    "\n",
    "The algorithm in the cell below iteratively picks one location from the test set and performs interpolation. Then forecast bias, the difference between the actual and predicted value, is calculated. Forecast Bias is helpful in understanding if our algorithm predictions are too low or too high (under- or overestimation). The following parameter is Root Mean Squared Error. This measure tells us more about outliers and considerable differences between predictions and actual values. We will see it in the last part of the tutorial.\n",
    "\n",
    "Your work with Poisson Kriging (or Kriging) will usually be the same:\n",
    "- prepare training and test data,\n",
    "- use training data to train the semivariogram model,\n",
    "- test different hyperparameters with a test set to find the optimal number of neighbors which are affecting your area of analysis,\n",
    "- forecast into unknown areas OR perform data smoothing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "number_of_obs = 4\n",
    "\n",
    "predslist = []\n",
    "for unknown_area in ar_test:\n",
    "    upts = pt_test[pt_test[:, 0] == unknown_area[0]]\n",
    "    upts = upts[:, 1:]\n",
    "    # [unknown block index, prediction, error]\n",
    "    try:\n",
    "        kriging_preds = area_to_area_pk(\n",
    "            semivariogram_model=semivariogram,\n",
    "            blocks=ar_train,\n",
    "            point_support=pt_train,\n",
    "            unknown_block=unknown_area[:-1],\n",
    "            unknown_block_point_support=upts,\n",
    "            number_of_neighbors=number_of_obs,\n",
    "            raise_when_negative_prediction=True,\n",
    "            raise_when_negative_error=True\n",
    "        )\n",
    "    except ValueError:\n",
    "        predslist.append([unknown_area[0], np.nan, np.nan, np.nan, np.nan])\n",
    "    else:\n",
    "        rmse = np.sqrt((kriging_preds[1] - unknown_area[-1])**2)\n",
    "        fb = unknown_area[-1] - kriging_preds[1]\n",
    "        kriging_preds.extend([rmse, fb])\n",
    "        predslist.append(kriging_preds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "kriged_predictions = np.array(predslist)\n",
    "pred_df = pd.DataFrame(data=kriged_predictions[:, 1:],\n",
    "                       index=kriged_predictions[:, 0],\n",
    "                       columns=['predicted', 'err', 'rmse', 'fb'])  # Store results in DataFrame"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5) Analyze Forecast Bias and Root Mean Squared Error of prediction\n",
    "\n",
    "The last step is the analysis of errors. We plot two histograms: forecast bias and root mean squared error then we calculate the base statistics of a dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show histograms of errors\n",
    "\n",
    "pred_df['fb'].plot.hist(bins=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_df['rmse'].plot.hist(bins=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Clarification**: Analysis of **Forecast Bias** and **Root Mean Squared Error** - their distribution and basic properties - could be a handy tool to analyze model performance. However, consider that table above is a single test case (realization) and can be misleading. The good idea is to repeat the test dozens of times with a different training/test set division each time. After this, we average results from multiple tests and get insight into how our model behaves.\n",
    "\n",
    "**Note 1**: Those results are not decisive. Our sample has been selected randomly, and there is a chance that it is not a spatially representative sample! (E.g., areas only from one region). The good idea is to repeat the experiment multiple times with other samples and average results to determine how well the model performs.\n",
    "\n",
    "**Note 2**: If we analyze errors’ statistics, we should consider not only the *mean* value of an error. Let’s take a look at different pieces of information:\n",
    "\n",
    "- Histograms clearly show us how dispersed and grouped are errors, and what’s most important, we directly see the worse predictions and how many of them are generated by our model.\n",
    "- What is plotted on a histogram, is described by statistics. We may check the max and the min error, but the true power comes when we analyze quartiles and standard deviation.\n",
    "- The standard deviation is a good measurement of our model’s variance, the less, the better.\n",
    "- Sometimes, we have to look into quartiles. A very high mean but relatively low median (or even the 3rd quartile) indicates that we have only a few wrong predictions, most of which are acceptable."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Where to go?\n",
    "\n",
    "### Next steps:\n",
    "\n",
    "* [Poisson Kriging - Centroid based]()\n",
    "* [Poisson Kriging - Area to Point]()\n",
    "\n",
    "### Additional materials:\n",
    "\n",
    "* [Semivariogram Regularization]()\n",
    "* [Blocks to points Ordinary Kriging interpolation]()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}